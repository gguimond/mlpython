https://github.com/machinelearningmindset/machine-learning-course
https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912
https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/
https://keon.io/deep-q-learning/
https://learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/
https://tiewkh.github.io/blog/deepqlearning-openaitaxi/
https://www.alexirpan.com/2018/02/14/rl-hard.html
https://towardsdatascience.com/real-world-examples-of-applied-machine-learning-from-ai-conference-4d4678700c6

All machine learning is AI, but not all AI is machine learning.

a) linear : ax+b=0

b)quadratic : ax^2+bx+c =0 and last but not the least

c)polynomial : (ax+b) ^n

y = a0 + a1x

a₀ and a₁ describe the shape of our line. a₀ is called the bias and a₁ is called a weight.

Fundamentally, classification is about predicting a label and regression is about predicting a quantity.

=================== Linear regression

Linear regression is a technique used to analyze a linear relationship between input variables and a single output variable. A linear relationship means that the data points tend to follow a straight line. Simple linear regression involves only a single input variable. 

=> Cost Function, error and squared error, Mean Squared Error (MSE)

=> minimizing the cost function : Ordinary Least Squares, Gradient descent (move the values of the coefficients and monitor whether the cost decreases or not.)

training set to train our model and a testing set to test its accuracy

=================== Overfitting

A model suffers from Overfitting when it has learned too much from the training data, and does not perform well in practice as a result
A model suffers from Underfitting when it has not learned enough from the training data, and does not perform well in practice as a result

=================== Regularization

When we look at a set of data, there are two main components: the underlying pattern and noise. We only want to match the pattern and not the noise

So if we were using a cost function CF, regularization might lead us to change it to CF + λ * R where R is some function of our weights and λ is a tuning parameter. The result is that models with higher weights (more complex) get penalized more. The tuning parameter basically lets us adjust the regularization to get better results. The higher the λ the less impact the weights have on the total cost.

Ridge regression forces weights to approach zero but will never cause them to be zero. This means that all the features will be represented in our model but overfitting will be minimized

Lasso regression is a type of regularization where the function R involves summing the absolute values of our weights. lasso regression can force weights to be zero. This means that our resulting model may not even consider some of the features

=================== Cross-Validation

Cross-validation assures a model is producing accurate results and comparing those results against other models

Holdout Method : The holdout cross-validation method involves removing a certain portion of the training data and using it as test data

K-Fold Cross Validation :  repeating the holdout method on k subsets of your dataset

Leave-P-Out Cross Validation (LPOCV) tests a model by using every possible combination of P test data points on a model. 



*********************************Supervised learning
=================== Decision Trees

Decision trees are a classifier in machine learning that allows us to make predictions based on previous data. They are like a series of sequential “if … then” statements you feed new data into to get a result.

A Classification Tree, like the one shown above, is used to get a result from a set of possible values. A Regression Tree is a decision tree where the result is a continuous value, such as the price of a car.

Splitting (Induction) : greedy algorithm:

Starting from the root, we create a split for each attribute.
For each created split, calculate the cost of the split.
Choose the split that costs the least.
Recurse into the sub-trees and continue from step 1.

Cost of Splitting : cost function cf example

Pruning :  it's beneficial to prune less important splits of a decision tree away. Pruning involves calculating the information gain of each ending sub-tree (the leaf nodes and their parent node), then removing the sub-tree with the least information gain

One hot encoder : 

╔════════════╦═════════════════╦════════╗ 
║ CompanyName Categoricalvalue ║ Price  ║
╠════════════╬═════════════════╣════════║ 
║ VW         ╬      1          ║ 20000  ║
║ Acura      ╬      2          ║ 10011  ║
║ Honda      ╬      3          ║ 50000  ║
║ Honda      ╬      3          ║ 10000  ║
╚════════════╩═════════════════╩════════╝

╔════╦══════╦══════╦════════╦
║ VW ║ Acura║ Honda║ Price  ║
╠════╬══════╬══════╬════════╬
║ 1  ╬ 0    ╬ 0    ║ 20000  ║
║ 0  ╬ 1    ╬ 0    ║ 10011  ║
║ 0  ╬ 0    ╬ 1    ║ 50000  ║
║ 0  ╬ 0    ╬ 1    ║ 10000  ║
╚════╩══════╩══════╩════════╝

=================== k-Nearest Neighbors

A classifier takes an already labeled data set, and then it trys to label new data points into one of the catagories.
To do this we look at the closest points (neighbors) to the object and the class with the majority of neighbors will be the class that we identify the object to be in.

Brute Force Method : calculating the Euclidean distance from the object being classified to each point in the set. The Euclidean distance is simply the length of a line segment that connects two points

K-D Tree Method : reducing the amount of times we calculate the Euclidean distance. The idea behind this method is that if we know that two data points are close to each other and we calculate the Euclidean distance to one of them and then we know that distance is roughly close to the other point

=================== Naive Bayes Classification

Naive Bayes is a classification technique that uses probabilities we already know to determine how to classify input. These probabilities are related to existing classes and what features they have. 

P(A|B) = P(A) P(B|A)
        -----------
           P(B)

The main thing we will assume is that features are independent. Assuming independence means that the probability of a set of features occurring given a certain class is the same as the product of all the probabilities of each individual feature occurring given that class.

Gaussian Model (Continuous) : Gaussian models assume features follow a normal distribution

Multinomial Model (Discrete) : Multinomial models are used when we are working with discrete counts. Specifically, we want to use them when we are counting how often a feature occurs.

Bernoulli Model (Discrete) : Unlike the multinomial case, here we are counting whether or not a feature occurred.

=================== Logistic Regression

Logistic regression is a method for binary classification. It works to divide points in a dataset into two distinct classes, or categories. For simplicity, let's call them class A and class B. The model will give us the probability that a given point belongs in category B. If it is low (lower than 50%), then we classify it in category A. Otherwise, it falls in class B. Logistic regression will instead create a sort of S-curve (using the sigmoid function) which will also help show certainty, since the output from logistic regression is not just a one or zero. 

Logistic regression is great for situations where you need to classify between two categories. Some good examples are accepted and rejected applicants and victory or defeat in a competition.

Logistic regression works using a linear combination of inputs, so multiple information sources can govern the output of the model. The parameters of the model are the weights of the various features, and represent their relative importance to the result. Logistic regression is, at its base, a transformation from a linear predictor to a probability between 0 and 1.

Multinomial Logistic Regression : where the output can be any digit from 0 to 9

=================== Linear Support Vector Machines

The point of SVM's are to try and find a line or hyperplane to divide a dimensional space which best classifies the data points. If we were trying to divide two classes A and B, we would try to best separate the two classes with a line. On one side of the line/hyperplane would be data from class A and on the other side would be from class B. 

This contrasts with the k-nearest neighbors algortihm, where we would have to calculate each data points nearest neighbors.

The algorithm chooses the line/hyperplane with the maximum margin. Maximizing the margin will give us the optimal line to classify the data. 
The data that is closest to the line is what determines the optimal line. These data points are called support vectors. The distance from these vectors to the hyperplane is called the margin. In general, the further those points are from the hyperplane, the greater the probability of correctly classifying the data.

non-linearly separable data = kernel trick. Basically, the kernel trick takes the points to a higher dimension to turn non-linearly separable data to linear separable data.



*********************************Unsupervised learning
=================== Clustering

Clustering is the process of grouping similar data and isolating dissimilar data. Clustering is used to identify potential groups in a data set while classification is used to match an input to an existing group.

K-Means : K-Means clustering attempts to divide a data set into K clusters using an iterative process. The first step is choosing a center point for each cluster. This center point does not need to correspond to an actual data point. The center points could be chosen at random or we could pick them if we have a good guess of where they should be. The second step is assigning each data point to a cluster. We do this by measuring the distance between a data point and each center point and choosing the cluster whose center point is the closest. Now that all the data points belong to a cluster, the third step is recomputing the center point of each cluster. This is just the average of all the data points belonging to the cluster. Now we just repeat the second and third step until the centers stop changing or only change slightly between iterations. 

K-Means clustering requires us to input the number of expected clusters which isn’t always easy to determine. It can also be inconsistent depending on where we choose the starting center points in the first step

Hierarchical : Hierarchical clustering imagines the data set as a hierarchy of clusters. We could start by making one giant cluster out of all the data points. Inside of this cluster, we find the two least similar sub-clusters and split them. This can be done by using an algorithm to maximize the inter-cluster distance. We continue to split the sub-clusters until every data point belongs to its own cluster or until we decide to stop. => top-down or divisive clustering

Alternatively, we could start by considering a cluster for every data point. The next step would be to combine the two closest clusters into a larger cluster. This can be done by finding the distance between every cluster and choosing the pair with the least distance between them. We would continue this process until we had a single cluster. => bottom-up or agglomerative clustering

Unlike K-Means, Hierarchical clustering is relatively slow so it doesn’t scale as well to large data sets. On the bright side, Hierarchical clustering is more consistent when you run it multiple times and doesn’t require you to know the number of expected clusters.

=================== Principal Component Analysis

Principal component analysis is one technique used to take a large list of interconnected variables and choose the ones that best suit a model. This process of focusing in on only a few variables is called dimensionality reduction, and helps reduce complexity of our dataset. At its root, principal component analysis summarizes data.

Feature elimination simply involves pruning features from a dataset we deem unnecessary. A downside of feature elimination is that we lose any potential information gained from the dropped features.

Feature extraction, however, creates new variables by combining existing features. At the cost of some simplicity or interpretability, feature extraction allows you to maintain all important information held within features.

Principal component analysis deals with feature extraction (rather than elimination) by creating a set of independent variables called principal components.
Techniques of performing principal component analysis range from arbitrarily selecting principal components, to automatically finding them until a variance is reached.



*********************************Deep learning
=================== Multi-layer Perceptron

A multilayer perceptron (MLP) is a deep, artificial neural network. A neural network is comprised of layers of nodes which activate at various levels depending on the previous layer's nodes.

Multilayer perceptron refers to a neural network with at least three layers of nodes, an input layer, some number of intermediate layers, and an output layer. Each node in a given layer is connected to every node in the adjacent layers. The input layer is just that, it is the way the network takes in data. The intermediate layer(s) are the computational machine of the network, they actually transform the input to the output. The output layer is the way that results are obtained from the neural network.

fully connected" layers : They require labeled sample data, so they carry out supervised learning. For each training sample, nodes activate according to stored weights of the previous layer. During training (and beyond), the weights will not be perfectly accurate, so they will need to change a little bit to meet the desired results. MLPs use a method called backpropagation to learn from training data.

A node is a single unit in a neural network. Nodes activate at different levels depending on a weighted sum of the previous layer's nodes. In practice, the actual activation is the result of a sigmoid function applied to this result, but we will skip over that detail here for simplicity. In MLPs, nodes activate based on all of the nodes in the previous layer.

When training a neural network, the expected output is a level of activation for each node in the output layer. From that information and the actual activation, we can find the cost at each node, and adjust the weights accordingly. The idea of backpropagation is to adjust the weights that determine each node's activation based on the cost.
In these early steps, it will have a high learning rate, making the weights more volatile. After a few iterations, it will be much more stable as it should need smaller adjustments. With that in mind, let's move forward one time step.

The lower the loss, the better a model (unless the model has over-fitted to the training data). The loss is calculated on training and validation and its interperation is how well the model is doing for these two sets. Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s).

Logits are the raw scores output by the last layer of a neural network. Before activation takes place.
Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes. it's a way of normalizing
The cross entropy is a summary metric: it sums across the elements.

The accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated.

Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.

=================== Convolutional Neural Networks

CNNs differ from other neural networks in that sequential layers are not necessarily fully connected. This means that a subset of the input neurons may only feed into a single neuron in the next layer. 
With other neural networks we might use vectors as inputs, but with CNNs we are typically working with images and other objects with many dimensions.

The architecture of a CNN can be broken down into an input layer, a set of hidden layers, and an output layer.

The hidden layers are where the magic happens. The hidden layers will break down our input image in order to identify features present in the image. The initial layers focus on low-level features such as edges while the later layers progressively get more abstract. At the end of all the layers, we have a fully connected layer with neurons for each of our classification values. What we end up with is a probability for each of the classification values.

convolutional layer : a convolution is some operation that acts on two input functions and produces an output function that combines the information present in the inputs. The first input will be our image and the second input will be some sort of filter such as a blur or sharpen. When we combine our image with the filter, we extract some information about the image.

The filter or kernel is one of the functions used in the convolution. The filter will likely have a smaller height and width than the input image and can be thought of as a window sliding over the image.

As the filter moves across the image, we are calculating values for the convolution output called a feature map. At each step, we multiply each entry in the image sample and filter elementwise and sum up all the products. This becomes an entry in the feature map.

we moved the filter one unit horizontally or one unit vertically from some previous position. This value is called the stride. 

 If we wanted the feature map to have the same height and width, we could pad the sample. This involves adding zero entries around the sample so that moving the filter keeps the dimensions of the original sample in the feature map. 

 The output of the convolution layer is a set of feature maps. 

 ReLU : The purpose of this layer is to introduce non-linearity into the system. Basically, real-world problems are rarely nice and linear so we want our CNN to account for this when it trains. 

 pooling layer : The purpose of pooling layers are to reduce the spatial size of the problem. This in turn reduces the number of parameters needed for processing and the total amount of computation in the CNN. max pooling => In max pooling, we slide a window over the input and take the max value in the window at each step.

 Fully connected layers are used to make the final classification in the CNN. Before moving to the first fully connected layer, we must flatten our input values into a one-dimensional vector that the layer can interpret.

 The output layer uses some function, such as softmax, to convert the neuron values into a probability distribution over our classes.

 Dropout Layer : is one of the most famous methods in order to prevent over-fitting. This operation randomly kills a portion of the neuron to stochastically force the neuron to learn more useful information.

 The problem with training CNNs and other deep learning models is that they are much more complex than the models we covered in earlier modules. This results in training being much more computationally expensive to the point where we would need specialized hardware like GPUs to run our code. 

 learning rate policy, placeholders, summaries

 =================== Autoencoders

 Autoencoders are a kind of neural networks which imitate their inputs and produce the exact information at their outputs. 
 Encoder and Decoder. The encoder transforms the input into a hidden space (hidden layer). The decoder then reconstructs the input information as the output.

 Undercomplete Autoencoders: In this type, the hidden dimension is smaller than the input dimension. Training such autoencoder lead to capturing the most prominent features.
  it is a feature extraction algorithm it helps us find a representation for our data and we can feed that representation to other algorithms, for example a classifier.

 Regularized Autoencoders, Sparse Autoencoders, Denoising Autoencoders , Contractive Autoencoders , Variational Autoencoders


 =================== Recurrent Neural Networks

A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model).

The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both.

RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector : this output vector’s contents are influenced not only by the input you just fed in, but also on the entire history of inputs you’ve fed in in the past. 


 =================== Deep reinforcement Q Learning

 A reinforcement learning task is about training an agent which interacts with its environment. The agent arrives at different scenarios known as states by performing actions. Actions lead to rewards which could be positive and negative.

The agent has only one purpose here – to maximize its total reward across an episode. This episode is anything and everything that happens between the first state and the last or terminal state within the environment. We reinforce the agent to learn to perform the best actions by experience. This is the strategy or policy.

Observation of the environment
Deciding how to act using some strategy
Acting accordingly
Receiving a reward or penalty
Learning from the experiences and refining our strategy
Iterate until an optimal strategy is found

Markov Decision Process (MDP) : we assume that each state follows a Markov property, i.e., each state depends solely on the previous state and the transition from that state to the current state.

Q Learning : Let’s say we know the expected reward of each action at every step. This would essentially be like a cheat sheet for the agent! Our agent will know exactly which action to perform. It will perform the sequence of actions that will eventually generate the maximum total reward. This total reward is also called the Q-value.

Q-value yielded from being at state s and performing action a is the immediate reward r(s,a) plus the highest Q-value possible from the next state s’. This is a recursive equation, we can start with making arbitrary assumptions for all q-values

approximate these Q-values with machine learning models such as a neural network? Well, this was the idea behind DeepMind’s algorithm that led to its acquisition by Google for 500 million dollars!

deep Q-learning : In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of all possible actions is generated as the output.

All the past experience is stored by the user in memory
The next action is determined by the maximum output of the Q-network
The loss function here is mean squared error of the predicted Q-value and the target Q-value – Q*. This is basically a regression problem.
the network is going to update its gradient using backpropagation to finally converge.

Challenges in Deep RL : Non-stationary or unstable target: the target is continuously changing with each iteration. In deep learning, the target variable does not change and hence the training is stable, which is just not true for RL. As we play out the game, we get to know more about the ground truth values of states and actions and hence, the output is also changing.

Target Network : instead of using one neural network for learning, we can use two. target network & prediction network. separate network to estimate the target. This target network has the same architecture as the function approximator but with frozen parameters. For every C iterations (a hyperparameter), the parameters from the prediction network are copied to the target network. This leads to more stable training because it keeps the target function fixed (for a while)

Experience Replay : Instead of running Q-learning on state/action pairs as they occur during simulation or the actual experience, the system stores the data discovered for [state, action, reward, next_state] – in a large table. During training, we could sample a random batch of 64 frames from the last 100,000 frames to train our network. This would get us a subset within which the correlation amongst the samples is low and will also provide better sampling efficiency.

‘exploration rate’ or ‘epsilon’ => agent will randomly select its action at first by a certain percentage

State Space
action space
Reward Table : {action: [(probability, nextstate, reward, done)]}

 Q-table & Q-values

 Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:

Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))
Where:

- α (alpha) is the learning rate (0<α≤1) - Just like in supervised learning settings, α is the extent to which our Q-values are being updated in every iteration.

- γ (gamma) is the discount factor (0≤γ≤1) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.

We are assigning (←), or updating, the Q-value of the agent's current state and action by first taking a weight (1−α) of the old Q-value, then adding the learned value. The learned value is a combination of the reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action.

Basically, we are learning the proper action to take in the current state by looking at the reward for the current state/action combo, and the max rewards for the next state. 

After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.

There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called ϵ "epsilon" to cater to this during training.

=> Deep RL Limits, benchmark

The rule-of-thumb is that except in rare cases, domain-specific algorithms work faster and better than reinforcement learning
Reward Function Design is Difficult
Even When Deep RL Works, It May Just Be Overfitting to Weird Patterns In the Environment
It’s hard to find cases where deep RL has created practical real world value.

=================== Real world example ML

DQN, AlphaGo, AlphaZero, Dota 2 (reinforcement learning), the parkour bot, reducing power center usage, and AutoML with Neural Architecture Search.

DeepMind AI Reduces Google Data Centre Cooling Bill by 40%

realistic speech synthesis Tacotron

Uber: Improving customer support with natural language processing and deep learning 
The Machine Learning team wanted to focus on making customer support representatives (CSRs) more effective by recommending the three most relevant solutions — essentially a ‘human-in-the-loop’ model architecture called Customer Obsession Ticket Assistant, or COTA.  (1) COTA v1 which converts a multi-class classification task into a ranking problem and (2) COTA v2 which used a deep learning approach called Encoder-Combiner-Decoder. Both models ingested the ticket, user, and trip information to suggest ticket classifications and reply templates (answers) for CSRs. COTA v2 was 20–30% more accurate than COTA v1 in their A/B tests. COTA v2 also reduced handling time by ~8% versus COTA v2’s ~15% reduction.

Zocdoc: Reverse engineering your AI prototype and the road to reproducibility (ZocDoc is an online medical care appointment booking service)
finding in-network physicians based on their insurance coverage. The ZocDoc team built an insurance card checker that allowed the patient to scan a picture of their insurance card, and then extracted the relevant details from the card to check whether a particular doctor and particular procedure was covered.
ZocDoc’s image recognition task was difficult because:
user-submitted images often have poor resolution and vary in dimension (due to a lack of formatting constraints) resulting in poor training data quality
insurance cards contain a plethora of other types of information and may sometimes repeat the member ID
the team had to quickly build a prototype then transform their process into a reproducible pipeline

completely tear down the infrastructure they had for the prototype because of scalability and reproducibility concerns.
However, their journey was one of constant iteration and frustration around the experience of data and model management.

Airbnb : Categorizing Listing Photos at Airbnb
Image Classification, categorization makes possible a simple home tour where photos with the same room type can be grouped together. For another, categorization makes it much easier to validate the number of certain rooms and check whether the basic room information is correct. 
Object Detection : we are able to verify the quality of the listings from hosts and make it much easier for guests to find homes with specific amenity needs

Spotify : To create Discover Weekly, there are three main types of recommendation models that Spotify employs:
Collaborative Filtering models (i.e. the ones that Last.fm originally used), which analyze both your behavior and others’ behaviors.
Natural Language Processing (NLP) models, which analyze text.
Audio models, which analyze the raw audio tracks themselves. : raw audio models take new songs into account => convolutional neural networks

Tesla : self-driving cars => imitation learning : Tesla’s engineers believe that by putting enough data from good human driving through a neural network, that network can learn how to directly predict the correct steering, braking and acceleration in most situations.”  we don’t know for sure that Tesla is using reinforcement learning to train Autopilot - so far the company has only said it’s using imitation learning. However, considering the vast amounts of driving data Tesla has available, and the success that others have been demonstrating with reinforcement learning, it seems likely that the company is combining the two techniques in its quest to teach a machine to match (or hopefully, exceed) human driving ability.

AlphaStar (deepMind) mastered StarCraft by using two different machine learning techniques. In supervised imitation learning, an AI examines a huge number of examples of something, and eventually learns how to recognize that something - in the classic example, if you show an AI a million photos of cats, it will learn to identify a cat, as opposed to a dog (which isn’t as easy as it sounds). The second technique, reinforcement learning, is a process of trial and error - an AI takes a random action, observes the effect, and learns which actions lead to the desired results. “Imitation learning followed by reinforcement learning is a one-two punch I suspect we could see a lot of in the future,” writes Eady.

Uber’s use of machine learning for ETAs for rides, estimated meal delivery times on UberEATS, computing optimal pickup locations, as well as for fraud detection.  to predict rider demand to ensure that “surge pricing”(short periods of sharp price increases to decrease rider demand and increase driver supply) will soon no longer be necessary.

Spam Filters, smart Email Categorization

plagiarism detection for regular text, Robo-readers

Mobile Check Deposits, the vast majority of major banks rely on technology developed by Mitek, which uses AI and ML to decipher and convert handwriting on checks into text via OCR. 
Fraud Prevention, FICO, the company that creates the well-known credit ratings used to determine creditworthiness, uses neural networks to predict fraudulent transactions.
Credit Decisions, FICO uses ML both in developing your FICO score, which most banks use to make credit decisions, and in determining the specific risk assessment for individual customers. MIT researchers found that machine learning could be used to reduce a bank’s losses on delinquent customers by up to 25%.

Facebook, Facebook discusses the use of artificial neural networks—ML algorithms that mimic the structure of the human brain—to power facial recognition software. DeepText, a text understanding engine that, the company claims “can understand with near-human accuracy the textual content of several thousand posts per second, spanning more than 20 languages.”

Pinterest uses computer vision, an application of AI where computers are taught to “see,” in order to automatically identify objects in images (or “pins”) and then recommend visually similar pins. Other applications of machine learning at Pinterest include spam prevention, search and discovery, ad performance and monetization, and email marketing.

Instagram uses machine learning to identify the contextual meaning of emoji : By algorithmically identifying the sentiments behind emojis, Instagram can create and auto-suggest emojis and emoji hashtags.

Snapchat facial filters, this technology is  powered by the 2015 acquisition of Looksery (for a rumored $150 million), a Ukranian company with patents on using machine learning to track movements in video.

AI chatbots => Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading Comprehension, Transfer Learning, Sequence to Sequence Model with multi-headed attention mechanism, Generative Adversarial Network, Self Learning based Sentiment Analysis and Deep Reinforcement Learning can help in Dialog Management for Conversational AI chatbot

Amazon : Amazon uses artificial neural networks to generate these product recommendations.

Machine learning is used for fraud prevention in online credit card transactions. By utilizing AI that can learn your purchasing habits, credit card processors minimize the probability of falsely declining your card while maximizing the probability of preventing somebody else from fraudulently charging it.

Google uses artificial neural networks to power voice search. 

Smart Personal Assistants : Alexa, an AI-powered personal assistant that accepts voice commands to create to-do lists, order items online, set reminders, and answer questions (via internet searches). Echo (and later, Dot) smart speakers that allow you to integrate Alexa into your living room and use voice commands to ask natural language questions, play music, order pizza, hail an Uber, and integrate with smart home devices.

casual chess players regularly use AI powered chess engines to analyze their games and practice tactics

bloggers often use mailing-list services that use ML to optimize reader engagement and open-rates.