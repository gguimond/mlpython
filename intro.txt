https://github.com/machinelearningmindset/machine-learning-course
https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912

a) linear : ax+b=0

b)quadratic : ax^2+bx+c =0 and last but not the least

c)polynomial : (ax+b) ^n

y = a0 + a1x

a₀ and a₁ describe the shape of our line. a₀ is called the bias and a₁ is called a weight.

Fundamentally, classification is about predicting a label and regression is about predicting a quantity.

=================== Linear regression

Linear regression is a technique used to analyze a linear relationship between input variables and a single output variable. A linear relationship means that the data points tend to follow a straight line. Simple linear regression involves only a single input variable. 

=> Cost Function, error and squared error, Mean Squared Error (MSE)

=> minimizing the cost function : Ordinary Least Squares, Gradient descent (move the values of the coefficients and monitor whether the cost decreases or not.)

training set to train our model and a testing set to test its accuracy

=================== Overfitting

A model suffers from Overfitting when it has learned too much from the training data, and does not perform well in practice as a result
A model suffers from Underfitting when it has not learned enough from the training data, and does not perform well in practice as a result

=================== Regularization

When we look at a set of data, there are two main components: the underlying pattern and noise. We only want to match the pattern and not the noise

So if we were using a cost function CF, regularization might lead us to change it to CF + λ * R where R is some function of our weights and λ is a tuning parameter. The result is that models with higher weights (more complex) get penalized more. The tuning parameter basically lets us adjust the regularization to get better results. The higher the λ the less impact the weights have on the total cost.

Ridge regression forces weights to approach zero but will never cause them to be zero. This means that all the features will be represented in our model but overfitting will be minimized

Lasso regression is a type of regularization where the function R involves summing the absolute values of our weights. lasso regression can force weights to be zero. This means that our resulting model may not even consider some of the features

=================== Cross-Validation

Cross-validation assures a model is producing accurate results and comparing those results against other models

Holdout Method : The holdout cross-validation method involves removing a certain portion of the training data and using it as test data

K-Fold Cross Validation :  repeating the holdout method on k subsets of your dataset

Leave-P-Out Cross Validation (LPOCV) tests a model by using every possible combination of P test data points on a model. 



*********************************Supervised learning
=================== Decision Trees

Decision trees are a classifier in machine learning that allows us to make predictions based on previous data. They are like a series of sequential “if … then” statements you feed new data into to get a result.

A Classification Tree, like the one shown above, is used to get a result from a set of possible values. A Regression Tree is a decision tree where the result is a continuous value, such as the price of a car.

Splitting (Induction) : greedy algorithm:

Starting from the root, we create a split for each attribute.
For each created split, calculate the cost of the split.
Choose the split that costs the least.
Recurse into the sub-trees and continue from step 1.

Cost of Splitting : cost function cf example

Pruning :  it's beneficial to prune less important splits of a decision tree away. Pruning involves calculating the information gain of each ending sub-tree (the leaf nodes and their parent node), then removing the sub-tree with the least information gain

One hot encoder : 

╔════════════╦═════════════════╦════════╗ 
║ CompanyName Categoricalvalue ║ Price  ║
╠════════════╬═════════════════╣════════║ 
║ VW         ╬      1          ║ 20000  ║
║ Acura      ╬      2          ║ 10011  ║
║ Honda      ╬      3          ║ 50000  ║
║ Honda      ╬      3          ║ 10000  ║
╚════════════╩═════════════════╩════════╝

╔════╦══════╦══════╦════════╦
║ VW ║ Acura║ Honda║ Price  ║
╠════╬══════╬══════╬════════╬
║ 1  ╬ 0    ╬ 0    ║ 20000  ║
║ 0  ╬ 1    ╬ 0    ║ 10011  ║
║ 0  ╬ 0    ╬ 1    ║ 50000  ║
║ 0  ╬ 0    ╬ 1    ║ 10000  ║
╚════╩══════╩══════╩════════╝

=================== k-Nearest Neighbors

A classifier takes an already labeled data set, and then it trys to label new data points into one of the catagories.
To do this we look at the closest points (neighbors) to the object and the class with the majority of neighbors will be the class that we identify the object to be in.

Brute Force Method : calculating the Euclidean distance from the object being classified to each point in the set. The Euclidean distance is simply the length of a line segment that connects two points

K-D Tree Method : reducing the amount of times we calculate the Euclidean distance. The idea behind this method is that if we know that two data points are close to each other and we calculate the Euclidean distance to one of them and then we know that distance is roughly close to the other point

=================== Naive Bayes Classification

Naive Bayes is a classification technique that uses probabilities we already know to determine how to classify input. These probabilities are related to existing classes and what features they have. 

P(A|B) = P(A) P(B|A)
        -----------
           P(B)

The main thing we will assume is that features are independent. Assuming independence means that the probability of a set of features occurring given a certain class is the same as the product of all the probabilities of each individual feature occurring given that class.

Gaussian Model (Continuous) : Gaussian models assume features follow a normal distribution

Multinomial Model (Discrete) : Multinomial models are used when we are working with discrete counts. Specifically, we want to use them when we are counting how often a feature occurs.

Bernoulli Model (Discrete) : Unlike the multinomial case, here we are counting whether or not a feature occurred.

=================== Logistic Regression

Logistic regression is a method for binary classification. It works to divide points in a dataset into two distinct classes, or categories. For simplicity, let's call them class A and class B. The model will give us the probability that a given point belongs in category B. If it is low (lower than 50%), then we classify it in category A. Otherwise, it falls in class B. Logistic regression will instead create a sort of S-curve (using the sigmoid function) which will also help show certainty, since the output from logistic regression is not just a one or zero. 

Logistic regression is great for situations where you need to classify between two categories. Some good examples are accepted and rejected applicants and victory or defeat in a competition.

Logistic regression works using a linear combination of inputs, so multiple information sources can govern the output of the model. The parameters of the model are the weights of the various features, and represent their relative importance to the result. Logistic regression is, at its base, a transformation from a linear predictor to a probability between 0 and 1.

Multinomial Logistic Regression : where the output can be any digit from 0 to 9

=================== Linear Support Vector Machines

The point of SVM's are to try and find a line or hyperplane to divide a dimensional space which best classifies the data points. If we were trying to divide two classes A and B, we would try to best separate the two classes with a line. On one side of the line/hyperplane would be data from class A and on the other side would be from class B. 

This contrasts with the k-nearest neighbors algortihm, where we would have to calculate each data points nearest neighbors.

The algorithm chooses the line/hyperplane with the maximum margin. Maximizing the margin will give us the optimal line to classify the data. 
The data that is closest to the line is what determines the optimal line. These data points are called support vectors. The distance from these vectors to the hyperplane is called the margin. In general, the further those points are from the hyperplane, the greater the probability of correctly classifying the data.

non-linearly separable data = kernel trick. Basically, the kernel trick takes the points to a higher dimension to turn non-linearly separable data to linear separable data.



*********************************Unsupervised learning
=================== Clustering

Clustering is the process of grouping similar data and isolating dissimilar data. Clustering is used to identify potential groups in a data set while classification is used to match an input to an existing group.

K-Means : K-Means clustering attempts to divide a data set into K clusters using an iterative process. The first step is choosing a center point for each cluster. This center point does not need to correspond to an actual data point. The center points could be chosen at random or we could pick them if we have a good guess of where they should be. The second step is assigning each data point to a cluster. We do this by measuring the distance between a data point and each center point and choosing the cluster whose center point is the closest. Now that all the data points belong to a cluster, the third step is recomputing the center point of each cluster. This is just the average of all the data points belonging to the cluster. Now we just repeat the second and third step until the centers stop changing or only change slightly between iterations. 

K-Means clustering requires us to input the number of expected clusters which isn’t always easy to determine. It can also be inconsistent depending on where we choose the starting center points in the first step

Hierarchical : Hierarchical clustering imagines the data set as a hierarchy of clusters. We could start by making one giant cluster out of all the data points. Inside of this cluster, we find the two least similar sub-clusters and split them. This can be done by using an algorithm to maximize the inter-cluster distance. We continue to split the sub-clusters until every data point belongs to its own cluster or until we decide to stop. => top-down or divisive clustering

Alternatively, we could start by considering a cluster for every data point. The next step would be to combine the two closest clusters into a larger cluster. This can be done by finding the distance between every cluster and choosing the pair with the least distance between them. We would continue this process until we had a single cluster. => bottom-up or agglomerative clustering

Unlike K-Means, Hierarchical clustering is relatively slow so it doesn’t scale as well to large data sets. On the bright side, Hierarchical clustering is more consistent when you run it multiple times and doesn’t require you to know the number of expected clusters.

=================== Principal Component Analysis

Principal component analysis is one technique used to take a large list of interconnected variables and choose the ones that best suit a model. This process of focusing in on only a few variables is called dimensionality reduction, and helps reduce complexity of our dataset. At its root, principal component analysis summarizes data.

Feature elimination simply involves pruning features from a dataset we deem unnecessary. A downside of feature elimination is that we lose any potential information gained from the dropped features.

Feature extraction, however, creates new variables by combining existing features. At the cost of some simplicity or interpretability, feature extraction allows you to maintain all important information held within features.

Principal component analysis deals with feature extraction (rather than elimination) by creating a set of independent variables called principal components.
Techniques of performing principal component analysis range from arbitrarily selecting principal components, to automatically finding them until a variance is reached.



*********************************Deep learning
=================== Multi-layer Perceptron

A multilayer perceptron (MLP) is a deep, artificial neural network. A neural network is comprised of layers of nodes which activate at various levels depending on the previous layer's nodes.

Multilayer perceptron refers to a neural network with at least three layers of nodes, an input layer, some number of intermediate layers, and an output layer. Each node in a given layer is connected to every node in the adjacent layers. The input layer is just that, it is the way the network takes in data. The intermediate layer(s) are the computational machine of the network, they actually transform the input to the output. The output layer is the way that results are obtained from the neural network.

fully connected" layers : They require labeled sample data, so they carry out supervised learning. For each training sample, nodes activate according to stored weights of the previous layer. During training (and beyond), the weights will not be perfectly accurate, so they will need to change a little bit to meet the desired results. MLPs use a method called backpropagation to learn from training data.

A node is a single unit in a neural network. Nodes activate at different levels depending on a weighted sum of the previous layer's nodes. In practice, the actual activation is the result of a sigmoid function applied to this result, but we will skip over that detail here for simplicity. In MLPs, nodes activate based on all of the nodes in the previous layer.

When training a neural network, the expected output is a level of activation for each node in the output layer. From that information and the actual activation, we can find the cost at each node, and adjust the weights accordingly. The idea of backpropagation is to adjust the weights that determine each node's activation based on the cost.
In these early steps, it will have a high learning rate, making the weights more volatile. After a few iterations, it will be much more stable as it should need smaller adjustments. With that in mind, let's move forward one time step.

The lower the loss, the better a model (unless the model has over-fitted to the training data). The loss is calculated on training and validation and its interperation is how well the model is doing for these two sets. Loss value implies how well or poorly a certain model behaves after each iteration of optimization. Ideally, one would expect the reduction of loss after each, or several, iteration(s).

Logits are the raw scores output by the last layer of a neural network. Before activation takes place.
Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes. it's a way of normalizing
The cross entropy is a summary metric: it sums across the elements.

The accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of misclassification is calculated.

Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.

=================== Convolutional Neural Networks

CNNs differ from other neural networks in that sequential layers are not necessarily fully connected. This means that a subset of the input neurons may only feed into a single neuron in the next layer. 
With other neural networks we might use vectors as inputs, but with CNNs we are typically working with images and other objects with many dimensions.

The architecture of a CNN can be broken down into an input layer, a set of hidden layers, and an output layer.

The hidden layers are where the magic happens. The hidden layers will break down our input image in order to identify features present in the image. The initial layers focus on low-level features such as edges while the later layers progressively get more abstract. At the end of all the layers, we have a fully connected layer with neurons for each of our classification values. What we end up with is a probability for each of the classification values.

convolutional layer : a convolution is some operation that acts on two input functions and produces an output function that combines the information present in the inputs. The first input will be our image and the second input will be some sort of filter such as a blur or sharpen. When we combine our image with the filter, we extract some information about the image.

The filter or kernel is one of the functions used in the convolution. The filter will likely have a smaller height and width than the input image and can be thought of as a window sliding over the image.

As the filter moves across the image, we are calculating values for the convolution output called a feature map. At each step, we multiply each entry in the image sample and filter elementwise and sum up all the products. This becomes an entry in the feature map.

we moved the filter one unit horizontally or one unit vertically from some previous position. This value is called the stride. 

 If we wanted the feature map to have the same height and width, we could pad the sample. This involves adding zero entries around the sample so that moving the filter keeps the dimensions of the original sample in the feature map. 

 The output of the convolution layer is a set of feature maps. 

 ReLU : The purpose of this layer is to introduce non-linearity into the system. Basically, real-world problems are rarely nice and linear so we want our CNN to account for this when it trains. 

 pooling layer : The purpose of pooling layers are to reduce the spatial size of the problem. This in turn reduces the number of parameters needed for processing and the total amount of computation in the CNN. max pooling => In max pooling, we slide a window over the input and take the max value in the window at each step.

 Fully connected layers are used to make the final classification in the CNN. Before moving to the first fully connected layer, we must flatten our input values into a one-dimensional vector that the layer can interpret.

 The output layer uses some function, such as softmax, to convert the neuron values into a probability distribution over our classes.

 Dropout Layer : is one of the most famous methods in order to prevent over-fitting. This operation randomly kills a portion of the neuron to stochastically force the neuron to learn more useful information.

 The problem with training CNNs and other deep learning models is that they are much more complex than the models we covered in earlier modules. This results in training being much more computationally expensive to the point where we would need specialized hardware like GPUs to run our code. 

 learning rate policy, placeholders, summaries

 =================== Autoencoders

 Autoencoders are a kind of neural networks which imitate their inputs and produce the exact information at their outputs. 
 Encoder and Decoder. The encoder transforms the input into a hidden space (hidden layer). The decoder then reconstructs the input information as the output.

 Undercomplete Autoencoders: In this type, the hidden dimension is smaller than the input dimension. Training such autoencoder lead to capturing the most prominent features.
  it is a feature extraction algorithm it helps us find a representation for our data and we can feed that representation to other algorithms, for example a classifier.

 Regularized Autoencoders, Sparse Autoencoders, Denoising Autoencoders , Contractive Autoencoders , Variational Autoencoders


 =================== Recurrent Neural Networks

A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model).

The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both.

RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector : this output vector’s contents are influenced not only by the input you just fed in, but also on the entire history of inputs you’ve fed in in the past. 

